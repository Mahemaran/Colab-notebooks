{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm/qb2Ecmr/rwTBH2sQ4/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahemaran/Colab-notebooks/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NLP**\n",
        "* Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables computers to understand, interpret, generate, and respond to human language in a meaningful way."
      ],
      "metadata": {
        "id": "Ej7N1dtM0NNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Corpus**\n",
        "* A corpus (plural: corpora) is a large collection of texts or spoken language that is systematically organized and used for language analysis, research, or training models in Natural Language Processing (NLP) and linguistics.\n",
        "* Real-world Data: It contains actual texts from books, articles, websites, spoken conversations, etc.\n",
        "* corpus = [\n",
        "    \"Machine learning is amazing.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"Deep learning models are used for text generation.\"\n",
        "]"
      ],
      "metadata": {
        "id": "uAjgK-As0e6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**\n",
        "* Tokenization is the process of splitting text into smaller units called tokens. These tokens can be words, subwords, sentences, or even characters, depending on the task. It is a fundamental step in Natural Language Processing (NLP) and machine learning\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7RXDrc3r1Wi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Tokenization**\n",
        "```\n",
        "Text: \"I love NLP!\"...\n",
        "Tokens: ['I', 'love', 'NLP', '!']\n",
        "```\n",
        "```\n",
        "Text: \"playing\"\n",
        "Tokens: ['play', '##ing']  # '##' indicates subword continuation\n",
        "```\n",
        "```\n",
        "Text: \"Hello world. NLP is fun.\"\n",
        "Tokens: ['Hello world.', 'NLP is fun.']\n",
        "```\n",
        "```\n",
        "Text: \"Hello world. NLP is fun.\"\n",
        "Tokens: ['Hello world.', 'NLP is fun.']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_iO58-V12-fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rule based Tokenization**\n",
        "```\n",
        "text = \"Hello, world!\"\n",
        "tokens = text.split()  # Basic split on spaces\n",
        "print(tokens)  # Output: ['Hello,', 'world!']\n",
        "```\n",
        "**NLTK**\n",
        "```\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Tokenization is important for NLP.\"\n",
        "print(word_tokenize(text))  \n",
        "# Output: ['Tokenization', 'is', 'important', 'for', 'NLP', '.']\n",
        "```\n",
        "```\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(\"Tokenization is crucial for BERT.\")\n",
        "print(tokens)\n",
        "# Output: ['token', '##ization', 'is', 'crucial', 'for', 'bert', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DqBUJCAM3Ka0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stop words**\n",
        "Stop words are common words in a language that are often filtered out or removed during text preprocessing in Natural Language Processing (NLP) tasks because they usually do not carry significant meaning or contribute to the analysis."
      ],
      "metadata": {
        "id": "sga9Mv5H3lN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples of stop words in English include:**\n",
        "* Articles: a, an, the\n",
        "* Prepositions: in, on, at\n",
        "* Conjunctions: and, or, but\n",
        "* Pronouns: he, she, it, they\n",
        "* Others: is, are, was, of, to, etc."
      ],
      "metadata": {
        "id": "MjmSZknv3-7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK (Natural Language Toolkit)**\n",
        "\n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Example text\n",
        "text = \"This is an example of stop word removal.\"\n",
        "# Tokenize text\n",
        "words = word_tokenize(text)\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Original:\", words)\n",
        "print(\"Filtered:\", filtered_words)\n",
        "#Output:\n",
        "Original: ['This', 'is', 'an', 'example', 'of', 'stop', 'word', 'removal', '.']\n",
        "Filtered: ['example', 'stop', 'word', 'removal', '.']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GcBwFQZb5aNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Padding**\n",
        "* the process of adding extra values (often zeros) to sequences so that all sequences have the same length.\n",
        "* it makes the input have same shape and size."
      ],
      "metadata": {
        "id": "kRLzJm6K5veT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common padding strategies:**\n",
        "* Pre-padding: Add values at the beginning of the sequence.\n",
        "* Post-padding: Add values at the end of the sequence."
      ],
      "metadata": {
        "id": "N7sHH1UW7QNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding in NLP (Tokenized Text)**\n",
        "```\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Sample sequences\n",
        "sequences = [\n",
        "    [1, 2, 3],\n",
        "    [4, 5],\n",
        "    [6]]\n",
        "# Pad sequences to a maximum length of 4\n",
        "padded_sequences = pad_sequences(sequences, maxlen=4, padding='pre')\n",
        "print(padded_sequences)\n",
        "# Output\n",
        "[[0 0 1 2 3]\n",
        " [0 0 0 4 5]\n",
        " [0 0 0 0 6]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ac6RL2nE7TfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One hot encoder**\n",
        "* One Hot Encoding is a technique used in machine learning and data preprocessing to convert categorical data into a binary matrix (0s and 1s)."
      ],
      "metadata": {
        "id": "ZBIKeUlw7lby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using OneHotEncoder from Scikit-learn**\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Example Data\n",
        "data = {'Animal': ['Cat', 'Dog', 'Bird', 'Cat']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and Transform\n",
        "encoded = encoder.fit_transform(df[['Animal']])\n",
        "\n",
        "# Convert to DataFrame\n",
        "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['Animal']))\n",
        "print(encoded_df)\n",
        "\n",
        "   Animal_Bird  Animal_Cat  Animal_Dog\n",
        "0          0.0         1.0         0.0\n",
        "1          0.0         0.0         1.0\n",
        "2          1.0         0.0         0.0\n",
        "3          0.0         1.0         0.0\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "P0t1N_1Q9D03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word embeddings**\n",
        "* Word embeddings are used to convert text (which is inherently non-numeric) into a numerical format that machine learning models can process effectively.\n",
        "* Word Embedding is a technique in Natural Language Processing (NLP) where words are represented as dense, continuous, and low-dimensional vectors in a numerical space. These vectors capture semantic relationships between words based on their meaning and context."
      ],
      "metadata": {
        "id": "uye7OU3I9lif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Gensim for Word2Vec**\n",
        "```\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load Pretrained Word2Vec Model\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Get Vector for a Word\n",
        "vector = model['king']\n",
        "print(\"Vector for 'king':\", vector)\n",
        "\n",
        "# Find Most Similar Words\n",
        "similar_words = model.most_similar('king', topn=5)\n",
        "print(\"Words similar to 'king':\", similar_words)\n",
        "\n",
        "Output:\n",
        "Words similar to 'king':\n",
        "[('queen', 0.78), ('prince', 0.75), ('monarch', 0.72), ('ruler', 0.69)]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "8KrnkQgkAZIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Keras Embedding Layer**\n",
        "\n",
        "```\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Example Parameters\n",
        "vocab_size = 1000  # Vocabulary size\n",
        "embedding_dim = 64  # Size of embedding vector\n",
        "input_length = 10   # Input sequence length\n",
        "\n",
        "# Model with Embedding Layer\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "```"
      ],
      "metadata": {
        "id": "-Sq2F13vAwcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF**\n",
        "* TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used in Natural Language Processing (NLP) and information retrieval to evaluate the importance of a word within a document or a collection of documents (corpus). It is widely used for text classification, clustering, and search engines.\n",
        "> TF-IDF is based on the idea that:\n",
        "* Term Frequency (TF): A word is more important if it appears frequently in a document.\n",
        "* Inverse Document Frequency (IDF): A word is less important if it appears frequently across many documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "RMUpNcHyBG-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**from sklearn.feature_extraction.text import TfidfVectorizer**\n",
        "```\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example Corpus\n",
        "documents = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Machine learning is fun.\",\n",
        "    \"Deep learning is a part of machine learning.\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform documents into TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names (terms)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to an array\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "# Show the terms and their corresponding TF-IDF values\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfidf_array, columns=terms)\n",
        "print(df)\n",
        "\n",
        "       deep  fun  is  learning  machine  part  the  love  a  of\n",
        "0  0.000000  0.0  0.0   0.577350  0.577350  0.0  0.0  0.577350  0.0  0.0\n",
        "1  0.000000  0.577350  0.577350  0.000000  0.577350  0.0  0.0  0.000000  0.0  0.0\n",
        "2  0.577350  0.0  0.577350  0.577350  0.577350  0.577350  0.0  0.0  0.577350  0.577350\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cxDDBSTAC3Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**\n",
        "* Stemming is a text preprocessing technique used in Natural Language Processing (NLP) to reduce words to their root form or stem."
      ],
      "metadata": {
        "id": "1g0efiPyDOpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example using Python:**\n",
        "\n",
        "\n",
        "```\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create an instance of the PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# List of words to stem\n",
        "words = [\"running\", \"runner\", \"ran\", \"happily\", \"happiness\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)\n",
        "\n",
        "Output:\n",
        "['run', 'runner', 'ran', 'happili', 'happi']\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "M01Jp3yVDjJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**\n",
        "* Lemmatization is a text preprocessing technique in Natural Language Processing (NLP) that aims to reduce words to their base or dictionary form (known as the lemma). Unlike stemming, which simply removes affixes from words, lemmatization involves more linguistic analysis to ensure that the resulting lemma is a valid word in the language."
      ],
      "metadata": {
        "id": "jDg-XCNzD4NM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization in Python using NLTK**\n",
        "```\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize some words\n",
        "words = [\"running\", \"better\", \"cats\", \"studies\", \"played\"]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # POS='v' for verb\n",
        "print(lemmatized_words)\n",
        "\n",
        "output:\n",
        "['run', 'better', 'cat', 'study', 'play']\n",
        "```"
      ],
      "metadata": {
        "id": "cfyRTzHnENps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POS**\n",
        "Part-of-Speech (POS) Tagging is a fundamental task in Natural Language Processing (NLP) that involves assigning a grammatical category (such as noun, verb, adjective, adverb, etc.) to each word in a sentence. POS tagging helps in understanding the syntactic structure of a sentence, which is crucial for various downstream NLP tasks like named entity recognition (NER), parsing, information retrieval, and machine translation."
      ],
      "metadata": {
        "id": "8m7MLSGxEXW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK for POS Tagging**\n",
        "```\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print POS tags\n",
        "for word, tag in tagged:\n",
        "    print(f'{word}: {tag}')\n",
        "\n",
        "output:\n",
        "Apple: NNP\n",
        "is: VBZ\n",
        "looking: VBG\n",
        "at: IN\n",
        "buying: VBG\n",
        "U.K.: NNP\n",
        "startup: NN\n",
        "for: IN\n",
        "$: $\n",
        "1: CD\n",
        "billion: NN\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y7ca3EBwE3w0"
      }
    }
  ]
}